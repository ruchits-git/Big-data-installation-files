Install Apache Hadoop:
======================

Step 1: Prerequisite


Java 8(OpenJDK) to be installed and set the JAVA_HOME environment variable in /home/datamaking/.bashrc file


echo $JAVA_HOME


To verify the java version you can use the following command:

java -version


From home directory(/home/datamaking), following directories are created

mkdir workarea

cd workarea

mkdir software

cd software

pwd

cd ~

pwd

Step 2: Create the SSH Key for password-less login (Press enter button when it asks you to enter a filename to save the key)

sudo apt-get install openssh-server openssh-client

ssh-keygen -t rsa -P ""

ls /home/datamaking/.ssh

Copy the generated ssh key to authorized keys

cat /home/datamaking/.ssh/id_rsa.pub >> /home/datamaking/.ssh/authorized_keys

ssh localhost

exit


Step 3: Download the Hadoop 3.3.6 Package/Binary file

https://hadoop.apache.org/releases.html


wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz


Move the file: hadoop-3.3.6.tar.gz into /home/datamaking/workarea/software directory

mv hadoop-3.3.6.tar.gz /home/datamaking/workarea/software

cd /home/datamaking/workarea/software

sudo tar -xzvf hadoop-3.3.6.tar.gz


Step 4: Add the HADOOP_HOME and JAVA_HOME paths in the bash file (.bashrc)

# HADOOP VARIABLES SETTINGS START HERE

export HADOOP_HOME=/home/datamaking/workarea/software/hadoop-3.3.6
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
export HADOOP_OPTS="-Djava.library.path=$HADOOP_COMMON_LIB_NATIVE_DIR"

# HADOOP VARIABLES SETTINGS END HERE

nano ~/.bashrc

source ~/.bashrc


hadoop version


Step 5: Create or Modifiy Hadoop configuration files

Now create/edit the configuration files in /home/datamaking/workarea/software/hadoop-3.3.6/etc/hadoop directory.

Edit hadoop-env.sh as follows,

ls /home/datamaking/workarea/software/hadoop-3.3.6/etc/hadoop/hadoop-env.sh

sudo nano /home/datamaking/workarea/software/hadoop-3.3.6/etc/hadoop/hadoop-env.sh

export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64



sudo mkdir -p /home/datamaking/workarea/software/hadoop_data/tmp

Edit core-site.xml as follows,

cd /home/datamaking/workarea/software/hadoop-3.3.6/etc/hadoop

sudo nano core-site.xml

	<property>
		<name>hadoop.tmp.dir</name>
		<value>/home/datamaking/workarea/software/hadoop_data/tmp</value>
		<description>Parent directory for other temporary directories.</description>
	</property>
	<property>
		<name>fs.defaultFS</name>
		<value>hdfs://localhost:9000</value>
		<description>The name of the default file system. </description>
	</property>


sudo mkdir -p /home/datamaking/workarea/software/hadoop_data/namenode

sudo mkdir -p /home/datamaking/workarea/software/hadoop_data/datanode

sudo chown -R datamaking:datamaking /home/datamaking/workarea/software


Edit hdfs-site.xml as follows,

sudo nano hdfs-site.xml

	<property>
		<name>dfs.namenode.name.dir</name>
		<value>/home/datamaking/workarea/software/hadoop_data/namenode</value>
	</property>
	<property>
		<name>dfs.datanode.data.dir</name>
		<value>/home/datamaking/workarea/software/hadoop_data/datanode</value>
	</property>
	<property>
		<name>dfs.replication</name>
		<value>1</value>
	</property>
	

Edit mapred-site.xml as follows,

sudo nano mapred-site.xml

	<property>
		<name>mapreduce.framework.name</name>
		<value>yarn</value>
	</property>

Edit yarn-site.xml as follows,

sudo nano yarn-site.xml

	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
	<property>
		<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
		<value>org.apache.hadoop.mapred.ShuffleHandler</value>
	</property>
	<property>
		<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
		<value>org.apache.hadoop.mapred.ShuffleHandler</value>
	</property>
	<property>
		<name>yarn.resourcemanager.webapp.address</name>
		<value>localhost:8088</value>
	</property>


Setting ownership for /home/datamaking/workarea/software

sudo chown -R datamaking:datamaking /home/datamaking/workarea/software


Step 6: Format the namenode

hdfs namenode -format

FYI.


2023-10-18 22:55:26,645 INFO namenode.FSImage: Allocated new BlockPoolId: BP-2119017789-127.0.1.1-1697649926632
2023-10-18 22:55:26,659 INFO common.Storage: Storage directory /home/datamaking/workarea/software/hadoop_data/namenode has been successfully formatted.
2023-10-18 22:55:26,768 INFO namenode.FSImageFormatProtobuf: Saving image file /home/datamaking/workarea/software/hadoop_data/namenode/current/fsimage.ckpt_0000000000000000000 using no compression
2023-10-18 22:55:26,890 INFO namenode.FSImageFormatProtobuf: Image file /home/datamaking/workarea/software/hadoop_data/namenode/current/fsimage.ckpt_0000000000000000000 of size 405 bytes saved in 0 seconds .
2023-10-18 22:55:26,897 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
2023-10-18 22:55:26,928 INFO namenode.FSNamesystem: Stopping services started for active state
2023-10-18 22:55:26,928 INFO namenode.FSNamesystem: Stopping services started for standby state
2023-10-18 22:55:26,932 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.
2023-10-18 22:55:26,932 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at datamakingws/127.0.1.1
************************************************************/
(base) datamaking@datamakingws:~/workarea/software/hadoop-3.3.6/etc/hadoop$ 



Start the NameNode daemon and DataNode daemon by using the scripts in the /sbin directory, provided by Hadoop.

start-dfs.sh


Start ResourceManager daemon and NodeManager daemon.


start-yarn.sh


Run jps command to check running hadoop JVM processes

jps

FYI.


(base) datamaking@datamakingws:~/workarea/software/hadoop-3.3.6/etc/hadoop$ jps
28833 ResourceManager
28626 SecondaryNameNode
28981 NodeManager
28263 NameNode
29352 Jps
28393 DataNode
(base) datamaking@datamakingws:~/workarea/software/hadoop-3.3.6/etc/hadoop$ 

#my info
ruchit@ruchit-HP-Pavilion-Laptop-15-eg2xxx:~/workarea/software/hadoop-3.3.6/etc/hadoop$ jps
16882 ResourceManager
16646 SecondaryNameNode
17401 Jps
17018 NodeManager
16283 NameNode
16412 DataNode
ruchit@ruchit-HP

Open your web browser and go to the below URL to browse the NameNode.

http://localhost:9870

http://18.225.32.72:9870


Open your web browser and go to the below URL to access the ResourceManager.


http://localhost:8088

http://18.225.32.72:8088


mr-jobhistory-daemon.sh start historyserver

